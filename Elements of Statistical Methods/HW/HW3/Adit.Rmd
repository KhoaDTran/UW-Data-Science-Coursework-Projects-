---
title: "Stat 311, HW2"
author: "Adit Jha"
date: "Wednesday Feb 5"
output: oilabs::lab_report
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(oilabs)
```

### Instructions
+ This homework is due on Wednesday Feb 5 by midnight. No late work is excused unless prior arrangements have been made with the instructor. The dropbox will remain open for 8 hours past the deadline. However, submissions made during this time will be penalized by at least 20% and maybe even as much as 50%.
+ Please answer ALL questions (including those that do not involve R) in the locations marked in this template. Remember to periodically **knit** your document to check that the output appears as you want it to; you will be turning in your knit html document. 
+ You will need to upload this document in CANVAS. Please be sure to check that your file was uploaded correctly. It does not hurt to screenshot verification of the upload as CANVAS can be glitchy on occasion.
+  Please answer the questions in the order in which they are posed. Write in complete sentences, and support your answers where asked. 

### Reading  (NOT OPTIONAL)
+ Sections 3.1, 3.2 in OpenIntro (4th Addition) 

* * *

### Exercise 1: Law of Large Numbers

a) According to genetic theory, there is a very close to even chance that both children in a two child family will be of the same gender. Here are two possibilities.

(i). 15 couples have two children. In 10 or more of these families, it will turn out that both children are of the same gender.

(ii). 30 couples have two children. In 20 or more of these families, it will turn out that both children are of the same gender.

Which possibility is more likely and why?

 The Situation (i) is more likely.
 Situation (i) is more likely because the Law of Large Numbers states that as you get more occurances of a specific situation, the probability of it happening converges to the average expected probability. Here we are told that according to genetic theory the probability of this situation is even so around 50%. Because 30 couples is a larger set of observations than 15, it is more likely for situation(i) will not converge to the average expected probability, since it is a smaller set of observations it will be more likely to be higher than the expected probability. We know this because the Law of Large numbers would indicate if we take a LOT of families, we would see the probability average out to around even, but with only 15 couples, the chance of getting 10 or more of them to have the same gender children will be more likely, and less likely for 20 or more couples out of 30 to have same gender children due to the larger observations set.
 
b) (It might help to work through problem 5 first before you takle this question) You flip a coin a large ($n$) number of times and observe the sequence of heads and tails. The following code ``simulates'' this experiment. 

```{r, echo=T,message=F}
n<-500     #number of students


set.seed(1234)
coin_outcomes<-c(0,0,1)
Coin_Flip<-sample(x=coin_outcomes,size=n,replace=T)

head(Coin_Flip)

```


(i) Calculate your *running mean* for the first 6 flips. 

   0, 0, 0, 0.25, 0.20, 0.17


(ii) In words, what does the running mean after 6 flips represent? Relate the running mean to what it tells you about the result of your coin flips.

   The running mean after 6 flips represents the proportion of heads that were flipped after 6 flips, which tells us that up until the 6 coin flips the result of heads as flips was the running average multiplied by the number of coin flips so far.

(iii) Give the number that the sequence of running means will *converge* (get close) to as $n$ increases? 

   The number that the sequence of running means wil converge to as n increases will be 0.50.


### Exercise 2: Calculating Probabilities

 Citrus trees are grown in ordered arrangements. A small [plot](https://drive.google.com/open?id=1KKPRdFzQweZm2vp3fcfQIKy-ogBpk8vw) has 3 rows with 3 trees in it as shown below. Each random experiment below has a sample space with equally likely outcomes. Describe the sample space, in particular, how many outcomes? Then use the ones favorable to the event of interest and calculate probabilities. 
	
a) A sample of 3 trees is obtained by choosing a row at random and then taking all 3 trees in that row. Calculate the probability that the tree labeled "1" will be selected under this sampling scheme? 

  P(Picking row that has tree 1) = 1/3
  
  The sample space would be S = {Row 1, Row 2, Row 3}. This sample space has 1 event that is favorable. 
 
         
b) A sample of 3 trees is obtained by randomly selecting a tree from each row. Calculate the probability that the tree labeled "1" will be selected under this sampling scheme. Highlight your final numerical answer. 

 P(Picking tree 1) = (Probability of picking tree 1 from Row 1) + (Probability of picking tree 1 from Row 2) + (Probability of picking tree 1 from Row 3) = 1/3 + 0 + 0 = $1/3$.
 
 The sample space would be S = {(1, 4, 7), (1, 4, 8), (1, 4, 9), (1, 5, 7), (1, 5, 8), (1, 5, 9), (1, 6, 7), (1, 6, 8), (1, 6, 9), (2, 4, 7), (2, 4, 8), (2, 4, 9), (2, 5, 7), (2, 5, 8), (2, 5, 9), (2, 6, 7), (2, 6, 8), (2, 6, 9), (3, 4, 7), (3, 4, 8), (3, 4, 9), (3, 5, 7), (3, 5, 8), (3, 5, 9), (3, 6, 7), (3, 6, 8), (3, 6, 9)}. This sampling has 9 favorable events.
        
c) A sample of 3 trees is obtained by first randomly choosing a starting tree (call it $start$) from 1, 2, 3. Every third tree thereafter is selected. For example, if $start=2$, then trees 2, 5 and 8 are selected. What is the probability that the tree labeled "1" will be selected under this sampling scheme. 

P(1 being selected) = P(1 being picked as start) = 1/3.

The sample space is S = {(1, 4, 7), (2, 5, 8), (3, 6, 9)}. The number of favorable events is 1.
         
d) The trees 3, 6 and 9 are easily accessible as they are right by a path and are chosen to be the sample. Can we calculate probabilities in this setting? Why or why not?
        
  Yes, you could calculate probabilities in this setting even when knowing the sample that is chosen because you can always calculate the probability of something not being in the sample and the probability of 3, 6, and 9 to be in the sample. Although the values are set and there is no chance for picking other numbers, these are still valid probabilities that could be calculated. For example, the probability that 1 is in this sample is 0, and the probability that 3 is in this sample is 1.
         
         
         
### Exercise 3: Probability Distributions

A box contains 100 marbles, some of which are red and the rest blue. A sample of 10 marbles is taken randomly (with replacement) from the box and the statistic: number of red marbles in the sample is calculated. 
 
 The probability model for this statistic is shown below. (Note: the probabilities should add to 1 -- any difference from 1 is due to round-off errors.)
 
|   values | probability (%) | 
|:--------:|  :----          |
|  0    |  0.6047         |
|	 1     |  4.0311         |  
|  2    | 12.0932         |
|  3   | 21.4991         |
|	 4     | 25.0823         |
|  5      | 20.0658         |
|	 6     | 11.1477         |
|  7   |  4.2467         |
| 8   |  1.0617         |
| 9   |  0.1573         |
|  10   |  0.0105         |
 
 a) Roughly, what is the shape of the probability model?
 	
  	The shape of the probability model is a bit right-skewed. 
 	
 b) Calculate the center (mean) of the probability model.
 
   The mean of the probability model is 4.000004.
  
```{r, echo=T}
  mean = (0 * 0.006047 + 1 * 0.040311 + 2 * 0.120932 +  3 * 0.214991 + 4  * 0.250823  + 5 * .200658 + 6 * 0.111477  + 7 * 0.042467  +  8 * 0.010617 + 9  * 0.001573  + 10 * 0.000105)
  mean
  
```
  
 c) Argue that 1.5 is a good guess for the standard deviation of the probability model. 
 
   1.5 is a good guess for the standard deviation of the probability model because if you take the calculated mean which is rounded to 4.00 then (4.00 - 1.5) = 2.5 and (4.00 + 1.5) = 5.5. Since the range of 2.5 and 5.5 represents the number of marbles that are red in a sample set of 10 models, we cannot have half marbles, so if you look at the rounded up range of 3 - 6 you get around 77% of the data falls within 1 standard deviation, which means 1.5 is a good standard deviation guess. Of course, the ideal number is 68% of your data should fall within 1 standard deviation of the mean, but since we have to round up in this case we will logically get a bit overestimated but still goes to show that because we get around the ideal % of data within 1 standard deviation of the mean, 1.5 is a good guess.
 
 d) Suppose you repeat the experiment of sampling 10 randomly with replacement $n$ times. Each time, you calculate the number of red marbles in your sample. Suppose you were to make a plot of the running means of the results, what would happen as $n$ increases? Any guess on what the running means would converge to? 
 
 
   My guess would be that if we made a plot of the running means of the results, the plot of the running means would straighten out, or converge, to the expected probability as n increases, because this graph would be about proportion not raw number of red marbles, where the Law of Large Numbers shows us that as the number of observations increases, the probability, aka running mean would converge to the expected probability of the sample which in this case is 0.4 that was found earlier in the problem, as we expect 4/10 marbles to be red.
   
### Exercise 4:  Conditional Probability

a) Suppose that 30% of computer owners use a Macintosh, 50% use Windows and 20% use Linux. Suppose that 65% of Mac users have succumbed to a virus, 82% of the Window users get the virus and 50% of the Linux users get the virus. We select a person at random and learn that her computer is infected with a virus. What is the probability that she is a Windows user?

The probability that she is a Windows user is 58.2%.

We can understand this by looking at the conditional probability that is being asked, which is what is the probability of the user having a windows given the condition she has a virus. This means we would want to use the defintion of conditional probability which is P(A and B) / P(B) which in this case is the probability of her having a virus and being a windows user divided by the probability of her having a virus. So I do (0.5 * 0. 82) / (.3 x .65+.5 x .82+.2 x .5) which gives us 0.582 which is 58.2%.


b) There are three cards. One is green on both sides, the second is red on both sides and the third is red on one side and green on the other. We choose a card at random and we see one side (also chosen at random). If the side we see is green, what is the probability that the other side is also green? Many people intuitively answer 1/2 but this is not the correct answer.  (Hint: Label one side of each card as 1 and the other as 2. An outcome is then $(x,y)$ where $x$ is the color and $y$ is the side of the card we see.)

The probability of the other side also being green is 2/3. 

This can be explained by the defintion of conditional probability, where we are looking at the probability of flip side being green GIVEN the current side is green. So we could do P(Green | Green) = P(Green & Green) / P(1 Green side), where we know that only 1/3 cards have both green sides and 3/6 sides have green in total, so simplified we see that (1/3)/(1/2) gives you 2/3, which is the conditional probability of the green card having it's back also be the green card.


### Exercise 5: Law of Large Numbers: R problem

In this question, you will use the simulation skills you learned in the lab to explore the consequences of the Law of Large Numbers.  The Law says that if we continue tossing a fair coin (say) for more and more tosses, we should get closer and closer to seeing ``heads`` 50% of the time. 

People generally interpret this statement as saying that the number of times that we see heads gets closer to half the number of flips. 

We will see through the simulations below that this is an incorrect interpretation of the Law of Large Numbers. It is the *proportion* of times that we see heads and not the *number* of times we see heads that that is being spoken about in the Law.  

One simple trick will make our lives easier in this problem. Instead of working with ``heads`` and ``tails`` as our coin outcomes, lets use `1` to denote ``heads`` and `0` to denote ``tails``. This will allow us to apply mathematical operations to our simulation results. 


### Part A 

Consider the following code.

```{r}
## Set a seed here
set.seed(500)
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
```

Recalling that `1` is heads and `0` is tails, how would you interpret the sum of the simulation results? How would you interpret the mean of the simulation results? Note that your interpretations should **not** use the words `sum` and `mean`. Also note that it may be helpful to pick a random seed; otherwise your answer will change each time you knit the document (but this question is not looking for a "correct" number of heads, so you may pick whatever seed you would like.) 

I would intrepret my results as follows: 
    - the first result I would interpret it as finding 28 heads in the sample set of the experiment running, and we can look at the size of the experiment and see that in this run 50 - 28 (our first result) tells us there were 22 tails that were flipped in the same sample set. This way we can tell how many heads we saw and how many tails in the duration of the experiment.
    - the second result I would interpret as the proportion of heads that were flipped out of the total runs in the sample set of 50 times, which means that 56% of the sample runs came out as heads since that is the first result divided by the size of the experiment.

### Part B

Let's explore what happens to the sum of the simulation results as the number of flips gets bigger and bigger. We can simulate 10000 flips, and then we can compute a **cumulative sum** (or running sum) using ``cumsum()``. 

```{r}
set.seed(311)
tosses = sample(coin_outcomes, size = 10000, replace=TRUE)
sum_so_far = cumsum(tosses)
```

Explore the ``sum_so_far`` object. The first element tells us our total sum after 1 flip. The second element tells us our total sum after 2 flips. The third element tells us our total sum after 3 flips, etc. Print out the 277th element of your ``sum_so_far`` object and interpret its value. What would you *expect* the sum to be after 277 flips? How far is your observed value from the expected value?

```{r}
sum_so_far[277]
```
The value we get when printing out the 277th vlaue in the sum_so_far table is 128. This value can be intepreted as the total heads we have seen after 277 flips in the experiment run. The sum that would be expected after 277 flips would be 138 as this would indicate very close to 50% of the 277 flips were heads. The observed value at the 277th run is 10 head flips less than the expected value at the 277th run.

### Part C

Now let's create an object called ``mean_so_far``, where element $i$ tells us the average result after $i$ flips. Note that ``(1:10000)`` simply creates a vector storing ``[1,2,3,4,....,10000]``, and R does element-by-element division. 

```{r}
mean_so_far = sum_so_far/(1:10000)

```

Explore the ``mean_so_far`` object. Print out the 112th element and interpret its value. Think about what you would *expect* the mean to be after 112 flips. How far is your observed value from the expected value?

```{r}
mean_so_far[112]
```
The 112th element in the mean_so_far object gives a value that can be intrepreted as the proportion of heads that have been flipped by the 112th run of the experiment. We would expect this value to be closer to 0.50 as that would indicate half the experiment runs up to the 112th run have been heads but our observed value is about 0.12 less than the expected result by the 112th run of the experiment.

### Part D
Let's visually explore the results using ``ggplot``. In order to use the ``ggplot`` syntax that you are used to, it will be useful to store all of the information you have so far in a dataframe. 

```{r}
coin_tosses <- data.frame(n=1:10000, expected_sum_so_far = 1:10000/2, 
                          sum_so_far = sum_so_far,
                          mean_so_far = mean_so_far, 
                          expected_mean_so_far = 0.5)
head(coin_tosses)
```

Using the new ``coin_tosses`` dataframe, make a plot using ``geom_point()`` where the x-axis is `n` (number of flips) and the y-axis is `sum_so_far - expected_sum_so_far`. You may mutate the dataframe to add the difference if you wish, or you can type the subtraction expression directly into ``ggplot``.

Does this plot relate to the number of heads or the proportion of heads?

```{r}
ggplot(coin_tosses, aes(x = n, y = sum_so_far - expected_sum_so_far)) + geom_point()
```

This plot relates to the number of heads as we are using the sum variable that keeps a count of how many heads we have seen so far till a specifc run count.

### Part E

Repeat part D, but now have the Y-axis be `mean_so_far - expected_mean_so_far`. 

Does this plot relate to the number of heads or the proportion of heads?

```{r}
ggplot(coin_tosses, aes(x = n, y = mean_so_far - expected_mean_so_far)) + geom_point()
```

This plot relates to the proportion of heads, as it looks at the average so far which is the proportion of head seen till a specific run count.

### Part F

Explore what happens if you change the random seed used to create ``tosses`` and repeat the analysis. Of the plots in part D and E (sum or mean), which one is more consistent when the random seed changes? We have provided a code chunk below for you to efficiently explore the results. You do not need to include the output of multiple random seeds in your submission, but you should include a description of what you learned from changing the random seeds. 

```{r}
set.seed(2800) ### Change this seed several times to see what happens
new_tosses = sample(coin_outcomes, size = 10000, replace=TRUE)
new_coin_tosses <- data.frame(n=1:10000, expected_sum_so_far = 1:10000/2, 
                              sum_so_far = cumsum(new_tosses), mean_so_far = cumsum(new_tosses)/(1:10000), expected_mean_so_far = 0.5)
ggplot(new_coin_tosses, aes(x = n, y = sum_so_far - expected_sum_so_far)) + geom_point()
ggplot(new_coin_tosses, aes(x = n, y = mean_so_far - expected_mean_so_far)) + geom_point()
### See how the plots change
```

As I changed the seed randomly, I saw that the second graph that looks at the proportion of heads up to a certain point in an experiment does not change, it looks exactly the same for the large numbers of flips no matter the seed. This has a converging line that is constant throughout my changing of the seed and it continued to stay the same as I kept changing the seeds. This differs from the first graph, which is not consistent with the random seed changes, as the different seeds that I set affected the plot of the sum difference drastically. We can observe that the fact that the proportion plot stays the same while the sum plot does not tells us that the Law of Large Numbers does not work for the sum of observations but the average proportions that converge to the expected probability, which we see occur with the consistency of our proportion plot regardless of seed changes.